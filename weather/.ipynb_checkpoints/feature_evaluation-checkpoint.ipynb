{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2_201_S.csv', '2_137_N.csv', '1_137_S.csv', '1_120_N.csv', '2_221_S.csv', '2_120_S.csv', '1_115_S.csv', '2_235_N.csv', '2_127_S.csv', '1_112_S.csv', '1_101_N.csv', '2_247_S.csv', '2_213_N.csv', '1_103_N.csv', '1_127_N.csv', '2_224_S.csv', '2_132_N.csv', '2_239_S.csv', '1_142_N.csv', '2_213_S.csv', '1_103_S.csv', '1_127_S.csv', '2_224_N.csv', '2_235_S.csv', '2_127_N.csv', '1_112_N.csv', '1_101_S.csv', '2_247_N.csv', '2_132_S.csv', '2_239_N.csv', '1_142_S.csv', '1_124_S.csv', '1_137_N.csv', '1_119_S.csv', '2_201_N.csv', '2_137_S.csv', '2_221_N.csv', '2_120_N.csv', '1_115_N.csv', '1_120_S.csv']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas\n",
    "dir_path = os.path.dirname(os.path.realpath('feature_selection.ipynb'))\n",
    "datasets_folder = dir_path +'/datasets'\n",
    "\n",
    "filenames = []\n",
    "for file in os.listdir(datasets_folder):\n",
    "    filenames.append(os.fsdecode(file))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def model_fit(model,X_train,X_test,y_train,y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_train)\n",
    "    #y_pred = np.ceil(y_pred)\n",
    "    mse = mean_squared_error(y_train,y_pred)\n",
    "    \n",
    "    #plt.plot(y_train,'ro')\n",
    "    #plt.plot(y_pred,'bo')\n",
    "    #plt.show()\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    #y_pred_test = np.ceil(y_pred_test)\n",
    "    \n",
    "    return model.feature_importances_,mse,y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8862287167203027\n",
      "4.043676663597375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rf():\n",
    "    sum_mse_train = 0\n",
    "    sum_mse_test = 0\n",
    "    for filename in filenames:\n",
    "        filename_new = filename.replace('.csv','')\n",
    "        line,stationID,stationDIR = filename_new.split('_')\n",
    "        cur_dir = datasets_folder+'/'+filename\n",
    "        data = pandas.read_csv(cur_dir,header=None)\n",
    "        data.columns = ['month','date','hour','temp','pressure','humidity',\n",
    "                       'wind_speed','wind_direction','clouds','weather_code',\n",
    "                       'minute','delay']\n",
    "        X = data[['hour','temp','pressure','humidity','wind_speed','clouds','weather_code','minute']]\n",
    "        y = data.iloc[:,11]\n",
    "        X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "        importances, mse, y_pred_test = model_fit(regr,X_train,X_test,y_train,y_test)\n",
    "        sum_mse_train += mse\n",
    "        sum_mse_test += mean_squared_error(y_test,y_pred_test)\n",
    "        #print('new station')\n",
    "        #print(importances)\n",
    "    return sum_mse_train/len(filenames),sum_mse_test/len(filenames)\n",
    "\n",
    "mse_train = 0\n",
    "mse_test = 0\n",
    "for i in range(10):\n",
    "    train,test = rf()\n",
    "    mse_train += train\n",
    "    mse_test += test\n",
    "print(mse_train/10)\n",
    "print(mse_test/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameter selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import json\n",
    "\n",
    "dict_para = {}\n",
    "for filename in filenames:\n",
    "    filename_new = filename.replace('.csv','')\n",
    "    line,stationID,stationDIR = filename_new.split('_')\n",
    "    cur_dir = datasets_folder+'/'+filename\n",
    "    data = pandas.read_csv(cur_dir,header=None)\n",
    "    data.columns = ['month','date','hour','temp','pressure','humidity',\n",
    "                   'wind_speed','wind_direction','clouds','weather_code',\n",
    "                   'minute','delay']\n",
    "    X = data[['hour','temp','pressure','humidity','wind_speed','clouds','minute']]\n",
    "    y = data.iloc[:,11]\n",
    "    X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "    tuned_parameters = [{'kernel':['rbf'],\n",
    "                         'gamma':[1e-3,1e-4,1e-2,0.1,1],\n",
    "                         'alpha':[0.1,1,10,100,1000]}]\n",
    "\n",
    "    clf = GridSearchCV(KernelRidge(),tuned_parameters,cv=5)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    dict_para[filename_new] = clf.best_params_\n",
    "model_param_dir = dir_path+'/models/model_param/krr_param.json'\n",
    "\n",
    "\n",
    "with open(model_param_dir, 'w') as fp:\n",
    "    json.dump(dict_para, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kernel ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9800765558384086\n",
      "4.099357077594718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "def krr_fit(X_train,X_test,y_train,y_test,key):\n",
    "    with open(model_param_dir, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    \n",
    "    kernel_param = data[key]['kernel']\n",
    "    gamma_param = data[key]['gamma']\n",
    "    alpha_param = data[key]['alpha']\n",
    "    \n",
    "    clf = KernelRidge(alpha=alpha_param,kernel = kernel_param,gamma = gamma_param)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    train_size = y_train_pred.shape\n",
    "    test_size = y_test_pred.shape\n",
    "    #fool_train = np.zeros(train_size)\n",
    "    #fool_test = np.zeros(test_size)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train,y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test,y_test_pred)\n",
    "    \n",
    "    return mse_train, mse_test\n",
    "\n",
    "def krr():\n",
    "    sum_mse_train = 0\n",
    "    sum_mse_test = 0\n",
    "    for filename in filenames:\n",
    "        filename_new = filename.replace('.csv','')\n",
    "        line,stationID,stationDIR = filename_new.split('_')\n",
    "        cur_dir = datasets_folder+'/'+filename\n",
    "        data = pandas.read_csv(cur_dir,header=None)\n",
    "        data.columns = ['month','date','hour','temp','pressure','humidity',\n",
    "                       'wind_speed','wind_direction','clouds','weather_code',\n",
    "                       'minute','delay']\n",
    "        X = data[['hour','temp','pressure','humidity','wind_speed','clouds','minute']]\n",
    "        y = data.iloc[:,11]\n",
    "        X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "        mse_train, mse_test = krr_fit(X_train,X_test,y_train,y_test,filename_new)\n",
    "        sum_mse_train += mse_train\n",
    "        sum_mse_test += mse_test\n",
    "        #print('new station')\n",
    "        #print(importances)\n",
    "    return sum_mse_train/len(filenames),sum_mse_test/len(filenames)\n",
    "\n",
    "mse_train = 0\n",
    "mse_test = 0\n",
    "for i in range(5):\n",
    "    train,test = krr()\n",
    "    mse_train += train\n",
    "    mse_test += test\n",
    "print(mse_train/5)\n",
    "print(mse_test/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM (currently doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def rnn_regr(X_train,X_test,y_train,y_test)\n",
    "    #Initialize the RNN\n",
    "    regressor = Sequential()\n",
    "    regressor.add(LSTM(units=50,return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # add a second LSTM layer\n",
    "    regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # add a third LSTM layer\n",
    "    regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # add a forth LSTM layer\n",
    "    regressor.add(LSTM(units = 50, return_sequences = False))\n",
    "    regressor.add(Dropout(0.2))\n",
    "\n",
    "    # add a output layer\n",
    "    regressor.add(Dense(units = 1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "    # Fitting the RNN to the Training set\n",
    "    regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "    \n",
    "    regressor.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1202, 1)\n",
      "(134, 1)\n",
      "(1326, 1)\n",
      "(148, 1)\n",
      "(2029, 1)\n",
      "(226, 1)\n",
      "(1979, 1)\n",
      "(220, 1)\n",
      "(1374, 1)\n",
      "(153, 1)\n",
      "(1385, 1)\n",
      "(154, 1)\n",
      "(1971, 1)\n",
      "(220, 1)\n",
      "(1224, 1)\n",
      "(136, 1)\n",
      "(1384, 1)\n",
      "(154, 1)\n",
      "(1913, 1)\n",
      "(213, 1)\n",
      "(2102, 1)\n",
      "(234, 1)\n",
      "(1265, 1)\n",
      "(141, 1)\n",
      "(1347, 1)\n",
      "(150, 1)\n",
      "(2022, 1)\n",
      "(225, 1)\n",
      "(1980, 1)\n",
      "(220, 1)\n",
      "(1369, 1)\n",
      "(153, 1)\n",
      "(1332, 1)\n",
      "(149, 1)\n",
      "(1285, 1)\n",
      "(143, 1)\n",
      "(1952, 1)\n",
      "(217, 1)\n",
      "(1369, 1)\n",
      "(153, 1)\n",
      "(1917, 1)\n",
      "(213, 1)\n",
      "(2004, 1)\n",
      "(223, 1)\n",
      "(1332, 1)\n",
      "(148, 1)\n",
      "(1285, 1)\n",
      "(143, 1)\n",
      "(1341, 1)\n",
      "(150, 1)\n",
      "(2166, 1)\n",
      "(241, 1)\n",
      "(1781, 1)\n",
      "(198, 1)\n",
      "(1168, 1)\n",
      "(130, 1)\n",
      "(1386, 1)\n",
      "(154, 1)\n",
      "(1229, 1)\n",
      "(137, 1)\n",
      "(2033, 1)\n",
      "(226, 1)\n",
      "(1945, 1)\n",
      "(217, 1)\n",
      "(1985, 1)\n",
      "(221, 1)\n",
      "(1980, 1)\n",
      "(221, 1)\n",
      "(1204, 1)\n",
      "(134, 1)\n",
      "(1392, 1)\n",
      "(155, 1)\n",
      "(1346, 1)\n",
      "(150, 1)\n",
      "(1342, 1)\n",
      "(150, 1)\n",
      "(2182, 1)\n",
      "(243, 1)\n",
      "(1987, 1)\n",
      "(221, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "def dataset_reshape(scaled):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    length = scaled.shape[0]\n",
    "    for i in range(60,length):\n",
    "        X_train.append(scaled[i-60:i,0])\n",
    "        y_train.append(scaled[i,0])\n",
    "    X_train,y_train = np.array(X_train),np.array(y_train)\n",
    "    X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "    return X_train,y_train\n",
    "\n",
    "for filename in filenames:\n",
    "    filename_new = filename.replace('.csv','')\n",
    "    line,stationID,stationDIR = filename_new.split('_')\n",
    "    cur_dir = datasets_folder+'/'+filename\n",
    "    data = pandas.read_csv(cur_dir,header=None)\n",
    "    data.columns = ['month','date','hour','temp','pressure','humidity',\n",
    "                   'wind_speed','wind_direction','clouds','weather_code',\n",
    "                   'minute','delay']\n",
    "    training_set = data.iloc[:,11:].values\n",
    "    #training_set_reshape = training_set.reshape(-1,1)\n",
    "    data_scaled = sc.fit_transform(training_set)\n",
    "    train, test = train_test_split(data_scaled,test_size=0.1)\n",
    "    X_train,y_train = dataset_reshape(train)\n",
    "    X_test,y_test = dataset_reshape(test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
